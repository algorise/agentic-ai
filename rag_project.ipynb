{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO72UVeorwY8SV+0H64NuIi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/algorise/agentic-ai/blob/main/rag_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9_sQf9PehBI"
      },
      "outputs": [],
      "source": [
        "pip install langchain pinecone-client google-generativeai openai tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dotenv"
      ],
      "metadata": {
        "id": "n8YiE2qdh5ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.getenv(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.getenv(\"PINECONE_API_KEY\")"
      ],
      "metadata": {
        "id": "R-foMgNMiOc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "# Create a new index or connect to an existing one\n",
        "index_name = \"online-rag-project\"\n",
        "\n",
        "pc.create_index(index_name,\n",
        "                dimension=768,\n",
        "                metric=\"cosine\",\n",
        "                spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "H58KZTOWif3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
      ],
      "metadata": {
        "id": "usFey1mLi_Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents\n",
        "loader = TextLoader(\"documents.txt\")  # Replace with your file\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "K78qnM8gqcwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "    index.upsert([(doc.metadata[\"source\"], vector, doc.page_content)])"
      ],
      "metadata": {
        "id": "6e6qsmXeql8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import GoogleGeminiFlash\n",
        "\n",
        "gemini_model = GoogleGeminiFlash(api_key=os.getenv(\"GOOGLE_API_KEY\"), temperature=0.7)"
      ],
      "metadata": {
        "id": "GjW5SW8mq00T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_model,\n",
        "    chain_type=\"stuff\",  # Other options: \"map_reduce\", \"refine\"\n",
        "    retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "3xoR0U_UrAJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the history of artificial intelligence?\"\n",
        "response = qa_chain.run(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "7DwmqolSrGL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}